#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Backup Autom√°tico do Banco de Dados
Cria backup do PostgreSQL e envia para S3/GCS
"""

import os
import sys
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
import gzip
import shutil

# Configura√ß√µes
BACKUP_DIR = Path("/app/backups") if os.path.exists("/app") else Path("./backups")
RETENTION_DAYS = int(os.getenv("BACKUP_RETENTION_DAYS", "30"))
DATABASE_URL = os.getenv("DATABASE_URL")


def ensure_backup_dir():
    """Garante que diret√≥rio de backup existe"""
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    print(f"üìÅ Diret√≥rio de backup: {BACKUP_DIR}")


def get_db_connection_params():
    """Extrai par√¢metros de conex√£o do DATABASE_URL"""
    # postgresql://user:password@host:port/dbname
    if not DATABASE_URL:
        raise ValueError("DATABASE_URL n√£o configurada!")

    import re

    match = re.match(r"postgresql://([^:]+):([^@]+)@([^:]+):(\d+)/(.+)", DATABASE_URL)

    if not match:
        raise ValueError("DATABASE_URL inv√°lida!")

    return {
        "user": match.group(1),
        "password": match.group(2),
        "host": match.group(3),
        "port": match.group(4),
        "database": match.group(5),
    }


def create_backup():
    """Cria backup do banco de dados"""
    print("\nüíæ Criando backup do banco de dados...")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = BACKUP_DIR / f"backup_{timestamp}.sql"
    backup_compressed = BACKUP_DIR / f"backup_{timestamp}.sql.gz"

    try:
        db_params = get_db_connection_params()

        # Comando pg_dump
        cmd = [
            "pg_dump",
            "-h",
            db_params["host"],
            "-p",
            db_params["port"],
            "-U",
            db_params["user"],
            "-d",
            db_params["database"],
            "-F",
            "p",  # Plain SQL
            "-f",
            str(backup_file),
        ]

        # Configurar senha via vari√°vel de ambiente
        env = os.environ.copy()
        env["PGPASSWORD"] = db_params["password"]

        # Executar pg_dump
        print(f"üîÑ Executando pg_dump...")
        result = subprocess.run(cmd, env=env, capture_output=True, text=True)

        if result.returncode != 0:
            print(f"‚ùå Erro ao executar pg_dump: {result.stderr}")
            return None

        print(f"‚úÖ Backup criado: {backup_file}")

        # Comprimir backup
        print(f"üîÑ Comprimindo backup...")
        with open(backup_file, "rb") as f_in:
            with gzip.open(backup_compressed, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)

        # Remover arquivo n√£o comprimido
        backup_file.unlink()

        # Verificar tamanho
        size_mb = backup_compressed.stat().st_size / (1024 * 1024)
        print(f"‚úÖ Backup comprimido: {backup_compressed} ({size_mb:.2f} MB)")

        return backup_compressed

    except Exception as e:
        print(f"‚ùå Erro ao criar backup: {e}")
        return None


def upload_to_s3(backup_file):
    """Upload do backup para AWS S3"""
    aws_key = os.getenv("AWS_ACCESS_KEY_ID")
    aws_secret = os.getenv("AWS_SECRET_ACCESS_KEY")
    s3_bucket = os.getenv("AWS_S3_BUCKET")

    if not all([aws_key, aws_secret, s3_bucket]):
        print("‚ö†Ô∏è Credenciais AWS n√£o configuradas, pulando upload S3")
        return False

    try:
        import boto3
        from botocore.exceptions import ClientError

        print(f"\n‚òÅÔ∏è Enviando para S3: {s3_bucket}...")

        s3_client = boto3.client(
            "s3", aws_access_key_id=aws_key, aws_secret_access_key=aws_secret
        )

        s3_key = f"backups/database/{backup_file.name}"

        s3_client.upload_file(str(backup_file), s3_bucket, s3_key)

        print(f"‚úÖ Backup enviado para S3: s3://{s3_bucket}/{s3_key}")
        return True

    except Exception as e:
        print(f"‚ùå Erro ao enviar para S3: {e}")
        return False


def upload_to_gcs(backup_file):
    """Upload do backup para Google Cloud Storage"""
    gcs_bucket = os.getenv("GCS_BUCKET")
    gcp_credentials = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

    if not all([gcs_bucket, gcp_credentials]):
        print("‚ö†Ô∏è Credenciais GCP n√£o configuradas, pulando upload GCS")
        return False

    try:
        from google.cloud import storage

        print(f"\n‚òÅÔ∏è Enviando para GCS: {gcs_bucket}...")

        storage_client = storage.Client()
        bucket = storage_client.bucket(gcs_bucket)

        blob_name = f"backups/database/{backup_file.name}"
        blob = bucket.blob(blob_name)

        blob.upload_from_filename(str(backup_file))

        print(f"‚úÖ Backup enviado para GCS: gs://{gcs_bucket}/{blob_name}")
        return True

    except Exception as e:
        print(f"‚ùå Erro ao enviar para GCS: {e}")
        return False


def cleanup_old_backups():
    """Remove backups antigos (mais de RETENTION_DAYS dias)"""
    print(f"\nüóëÔ∏è Limpando backups antigos (> {RETENTION_DAYS} dias)...")

    cutoff_date = datetime.now() - timedelta(days=RETENTION_DAYS)
    removed = 0

    for backup_file in BACKUP_DIR.glob("backup_*.sql.gz"):
        # Extrair data do nome do arquivo: backup_YYYYMMDD_HHMMSS.sql.gz
        try:
            date_str = backup_file.stem.split("_")[1]  # YYYYMMDD
            backup_date = datetime.strptime(date_str, "%Y%m%d")

            if backup_date < cutoff_date:
                backup_file.unlink()
                removed += 1
                print(f"üóëÔ∏è Removido: {backup_file.name}")
        except:
            continue

    if removed > 0:
        print(f"‚úÖ {removed} backup(s) antigo(s) removido(s)")
    else:
        print("‚úÖ Nenhum backup antigo para remover")


def list_backups():
    """Lista todos os backups dispon√≠veis"""
    print("\nüìã Backups dispon√≠veis:")

    backups = sorted(BACKUP_DIR.glob("backup_*.sql.gz"), reverse=True)

    if not backups:
        print("‚ùå Nenhum backup encontrado")
        return

    for i, backup in enumerate(backups[:10], 1):  # Mostrar √∫ltimos 10
        size_mb = backup.stat().st_size / (1024 * 1024)
        mtime = datetime.fromtimestamp(backup.stat().st_mtime)
        print(
            f"{i}. {backup.name} - {size_mb:.2f} MB - {mtime.strftime('%Y-%m-%d %H:%M:%S')}"
        )


def main():
    """Fun√ß√£o principal"""
    print("=" * 60)
    print("üíæ GestaoVersus - Backup Autom√°tico do Banco de Dados")
    print("=" * 60)
    print(f"üìÖ Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Garantir diret√≥rio de backup
    ensure_backup_dir()

    # Criar backup
    backup_file = create_backup()

    if not backup_file:
        print("\n‚ùå Falha ao criar backup!")
        sys.exit(1)

    # Upload para cloud (S3 ou GCS)
    backup_storage = os.getenv("BACKUP_STORAGE", "local")

    if backup_storage == "s3":
        upload_to_s3(backup_file)
    elif backup_storage == "gcs":
        upload_to_gcs(backup_file)
    elif backup_storage == "both":
        upload_to_s3(backup_file)
        upload_to_gcs(backup_file)
    else:
        print("‚ÑπÔ∏è Backup armazenado localmente apenas")

    # Limpar backups antigos
    cleanup_old_backups()

    # Listar backups dispon√≠veis
    list_backups()

    print("\n" + "=" * 60)
    print("‚úÖ Backup conclu√≠do com sucesso!")
    print("=" * 60)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Backup cancelado pelo usu√°rio")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erro inesperado: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
